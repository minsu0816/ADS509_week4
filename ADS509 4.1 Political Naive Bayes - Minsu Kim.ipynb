{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes on Political Text\n",
    "\n",
    "In this notebook we use Naive Bayes to explore and classify political data. See the `README.md` for full details. You can download the required DB from the shared dropbox or from blackboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "\n",
    "# Feel free to include your text patterns functions\n",
    "#from text_functions_solutions import clean_tokenize, get_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "tw_punct = punctuation - {\"#\"}\n",
    "\n",
    "# Two useful regex\n",
    "whitespace_pattern = re.compile(r\"\\s+\")\n",
    "hashtag_pattern = re.compile(r\"^#[0-9a-zA-Z]+\")\n",
    "\n",
    "def remove_stop(tokens) :\n",
    "    return([word for word in tokens if word not in sw])\n",
    "\n",
    "def remove_punctuation(text, punct_set=tw_punct) : \n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "def tokenize(text) : \n",
    "    \"\"\" Splitting on whitespace rather than the book's tokenize function. That \n",
    "        function will drop tokens like '#hashtag' or '2A', which we need for Twitter. \"\"\"\n",
    "    \n",
    "    return([t for t in whitespace_pattern.split(text) if t])\n",
    "\n",
    "def prepare(text, pipeline) : \n",
    "    tokens = str(text)\n",
    "    \n",
    "    for transform in pipeline : \n",
    "        tokens = transform(tokens)\n",
    "        \n",
    "    return(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity, and num_tokens most common\n",
    "        tokens. Return a list of \n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose :        \n",
    "        print(f\"There are {len(tokens)} tokens in the data.\")\n",
    "        print(f\"There are {len(set(tokens))} unique tokens in the data.\")\n",
    "        print(f\"There are {len(''.join(tokens))} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {len(set(tokens))/len(tokens):.3f} in the data.\")\n",
    "    \n",
    "        counts = Counter(tokens)\n",
    "\n",
    "        if num_tokens > 0 :\n",
    "            print(counts.most_common(num_tokens))\n",
    "        \n",
    "    return([len(tokens),\n",
    "           len(set(tokens)),\n",
    "           len(\"\".join(tokens)),\n",
    "           len(set(tokens))/len(tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_db = sqlite3.connect('2020_Conventions.db')\n",
    "convention_cur = convention_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1b5ab073490>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print tables in convention database\n",
    "sql_query = \"\"\"SELECT name FROM sqlite_master  \n",
    "  WHERE type='table';\"\"\"\n",
    "\n",
    "convention_cur.execute(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('conventions',)]\n"
     ]
    }
   ],
   "source": [
    "print(convention_cur.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Exploratory Naive Bayes\n",
    "\n",
    "We'll first build a NB model on the convention data itself, as a way to understand what words distinguish between the two parties. This is analogous to what we did in the \"Comparing Groups\" class work. First, pull in the text \n",
    "for each party and prepare it for use in Naive Bayes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some random entries and see if they look right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipeline = [str.lower, remove_punctuation,tokenize]\n",
    "convention_data = []\n",
    "query_results = convention_cur.execute(\n",
    "                            '''\n",
    "                                SELECT text, party\n",
    "                                FROM conventions\n",
    "                                WHERE speaker != \"Unknown\"\n",
    "                            ''')\n",
    "for row in query_results :\n",
    "    text = ' '.join(prepare(row[0], pipeline=my_pipeline))\n",
    "    convention_data.append([text, row[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this time next year', 'Democratic'],\n",
       " ['delaware', 'Republican'],\n",
       " ['china would prefer joe biden', 'Republican'],\n",
       " ['i said this before if i gave away everything that i have today it would not equal 1 of what i was given when i came to this great country of ours the gift of freedom right now it is up to us to decide our fate and to choose freedom over oppression president trump he’s fighting the forces of anarchy and communism and i know he will continue to do just that what about his opponent and the rest of the dc swamp i have no doubt that we’ll hand the country over to those dangerous forces you and i will decide and here’s what i’ve decided my decision is very easy i choose president trump because i choose america i choose freedom i still hear my dad there is no other place to go thank you and may the good lord bless america',\n",
       "  'Republican'],\n",
       " ['president trump says “this is how we fix it” and i thought “well that’s a simple solution” there’s no other president that could have done it there’s no one that has even tried to do it president trump is a doer he appreciates every one of us and i know he does i’ve seen it when he said make america great again that was his task that wasn’t his slogan that was his task and every hat you see that says maga on it that’s what your president is doing for you thank you mr president for keeping the promises that you made',\n",
       "  'Republican'],\n",
       " ['today i channel my warrior roots by battling human trafficking i’ve been able to lead some of the largest trafficking prosecutions in america and traveled to foreign countries working with law enforcement and ngos to dismantle trafficking networks and rescue people from the most brutal conditions imaginable young girls and women sold into sex slavery young boys and men forced into labor servitude illegal adoptions organ harvesting human life utterly debased',\n",
       "  'Republican'],\n",
       " ['if we live in hope we open our souls to the power of love we’ve been taught to love our neighbors as ourselves as individuals and as a nation however we fail at following that commandment more often than we succeed but when we fail we must try again and again and again for only in trial is progress possible from jamestown forward our story has become fuller and fairer because of people who share a conviction that dr king articulated on that sunday half a century ago the arc of the moral universe is long but it bends toward justice bending that arc requires all of us it requires we the people and it requires a president of the united states with empathy grace a big heart and an open mind joe biden will be such a president with our voices and our votes let us now write the next chapter of the american story one of hope of love of justice if we do so we might just save our country and our souls',\n",
       "  'Democratic'],\n",
       " ['hi my name is brayden harrington and i am 13 years old and without joe biden i wouldn’t be talking to you today about a few months ago i met him in new hampshire he told me that we were members of the same club we stutter it was really amazing to hear that someone like me became vice president he told me about a book of poems by yeats he would read out loud to practice he showed me how he marks his addresses to make them easier to say out loud so i did the same thing today and now i’m here talking to you today about the future about our future',\n",
       "  'Democratic'],\n",
       " ['when she was second lady jill told me that she would like to continue teaching at community college and i said “that’s insane you cannot possibly do that” dr',\n",
       "  'Democratic'],\n",
       " ['it is the most pressing issue of our time and we deserve to be treated as such',\n",
       "  'Democratic']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(convention_data,k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that looks good, we now need to make our function to turn these into features. In my solution, I wanted to keep the number of features reasonable, so I only used words that occur at least `word_cutoff` times. Here's the code to test that if you want it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a word cutoff of 5, we have 2482 as features in the model.\n"
     ]
    }
   ],
   "source": [
    "word_cutoff = 5\n",
    "\n",
    "tokens = [w for t, p in convention_data for w in t.split()]\n",
    "\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "\n",
    "feature_words = set()\n",
    "\n",
    "for word, count in word_dist.items() :\n",
    "    if count > word_cutoff :\n",
    "        feature_words.add(word)\n",
    "        \n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words)} as features in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_features(text,fw) :\n",
    "    \"\"\"Given some text, this returns a dictionary holding the\n",
    "       feature words.\n",
    "       \n",
    "       Args: \n",
    "            * text: a piece of text in a continuous string. Assumes\n",
    "            text has been cleaned and case folded.\n",
    "            * fw: the *feature words* that we're considering. A word \n",
    "            in `text` must be in fw in order to be returned. This \n",
    "            prevents us from considering very rarely occurring words.\n",
    "        \n",
    "       Returns: \n",
    "            A dictionary with the words in `text` that appear in `fw`. \n",
    "            Words are only counted once. \n",
    "            If `text` were \"quick quick brown fox\" and `fw` = {'quick','fox','jumps'},\n",
    "            then this would return a dictionary of \n",
    "            {'quick' : True,\n",
    "             'fox' :    True}\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    text = set(text.split())\n",
    "    text = text.intersection(fw)\n",
    "    \n",
    "    ret_dict = dict()\n",
    "    \n",
    "    for word in text :\n",
    "        ret_dict[word] = True\n",
    "    \n",
    "    return(ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14736/2024516786.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m assert(conv_features(\"donald is the president\",feature_words)==\n\u001b[0m\u001b[0;32m      3\u001b[0m        {'donald':True,'president':True})\n\u001b[0;32m      4\u001b[0m assert(conv_features(\"people are american in america\",feature_words)==\n\u001b[0;32m      5\u001b[0m                      {'america':True,'american':True,\"people\":True})\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert(len(feature_words)>0)\n",
    "assert(conv_features(\"donald is the president\",feature_words)==\n",
    "       {'donald':True,'president':True})\n",
    "assert(conv_features(\"people are american in america\",feature_words)==\n",
    "                     {'america':True,'american':True,\"people\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build our feature set. Out of curiosity I did a train/test split to see how accurate the classifier was, but we don't strictly need to since this analysis is exploratory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': True, 'donald': True, 'president': True, 'the': True}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_features(\"donald is the president\", feature_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(conv_features(text,feature_words), party) for (text, party) in convention_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20220507)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "test_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.454\n"
     ]
    }
   ],
   "source": [
    "test_set, train_set = featuresets[:test_size], featuresets[test_size:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                 radical = True           Republ : Democr =     38.0 : 1.0\n",
      "                   media = True           Republ : Democr =     35.9 : 1.0\n",
      "                   votes = True           Democr : Republ =     22.8 : 1.0\n",
      "             enforcement = True           Republ : Democr =     18.4 : 1.0\n",
      "                   crime = True           Republ : Democr =     17.2 : 1.0\n",
      "                 destroy = True           Republ : Democr =     16.2 : 1.0\n",
      "                freedoms = True           Republ : Democr =     16.2 : 1.0\n",
      "                   china = True           Republ : Democr =     15.4 : 1.0\n",
      "                  earned = True           Republ : Democr =     14.1 : 1.0\n",
      "                  defund = True           Republ : Democr =     13.0 : 1.0\n",
      "                  lowest = True           Republ : Democr =     13.0 : 1.0\n",
      "              prosperity = True           Republ : Democr =     13.0 : 1.0\n",
      "                   taxes = True           Republ : Democr =     12.8 : 1.0\n",
      "                    mike = True           Republ : Democr =     12.2 : 1.0\n",
      "                 beliefs = True           Republ : Democr =     12.0 : 1.0\n",
      "                religion = True           Republ : Democr =     12.0 : 1.0\n",
      "                 violent = True           Republ : Democr =     12.0 : 1.0\n",
      "                      50 = True           Republ : Democr =     11.6 : 1.0\n",
      "                 african = True           Republ : Democr =     10.9 : 1.0\n",
      "                received = True           Republ : Democr =     10.9 : 1.0\n",
      "                 climate = True           Democr : Republ =     10.9 : 1.0\n",
      "                  record = True           Republ : Democr =     10.0 : 1.0\n",
      "               countries = True           Republ : Democr =      9.9 : 1.0\n",
      "                   enemy = True           Republ : Democr =      9.9 : 1.0\n",
      "                    seem = True           Republ : Democr =      9.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a little prose here about what you see in the classifier. Anything odd or interesting?\n",
    "\n",
    "### My Observations\n",
    "\n",
    "My observation was that the classifier accuracy was lower than expected. \n",
    "It's interesting to see how the most informative features are mostly used by Republicans and the only words that were more Democratic dominated were 'votes' and 'climate' (which makes sense to me).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classifying Congressional Tweets\n",
    "\n",
    "In this part we apply the classifer we just built to a set of tweets by people running for congress\n",
    "in 2018. These tweets are stored in the database `congressional_data.db`. That DB is funky, so I'll\n",
    "give you the query I used to pull out the tweets. Note that this DB has some big tables and \n",
    "is unindexed, so the query takes a minute or two to run on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_db = sqlite3.connect(\"congressional_data.db\")\n",
    "cong_cur = cong_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cong_cur.execute(\n",
    "        '''\n",
    "           SELECT DISTINCT \n",
    "                  cd.candidate, \n",
    "                  cd.party,\n",
    "                  tw.tweet_text\n",
    "           FROM candidate_data cd \n",
    "           INNER JOIN tweets tw ON cd.twitter_handle = tw.handle \n",
    "               AND cd.candidate == tw.candidate \n",
    "               AND cd.district == tw.district\n",
    "           WHERE cd.party in ('Republican','Democratic') \n",
    "               AND tw.tweet_text NOT LIKE '%RT%'\n",
    "        ''')\n",
    "\n",
    "results = list(results) # Just to store it, since the query is time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = []\n",
    "\n",
    "remove_url = re.compile(r'http\\S+')\n",
    "\n",
    "for candidate, party, text in results :\n",
    "    text = text.decode(\"utf-8\")\n",
    "    if text.startswith(\"b\"):\n",
    "        text = text[1: ]\n",
    "    text = remove_url.sub(\"\", text)\n",
    "\n",
    "    tweet_data.append([\"\".join(prepare(text, pipeline=my_pipeline)), party])\n",
    "\n",
    "# Now fill up tweet_data with sublists like we did on the convention speeches.\n",
    "# Note that this may take a bit of time, since we have a lot of tweets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of tweets here. Let's take a random sample and see how our classifer does. I'm guessing it won't be too great given the performance on the convention speeches..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20201014)\n",
    "\n",
    "tweet_data_sample = random.choices(tweet_data,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's our (cleaned) tweet: earliertodayispokeonthehousefloorabtprotectinghealthcareforwomenandpraisedppmarmontefortheirworkonthecentralcoast\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: gotribe#rallytogether\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: apparentlytrumpthinksitsjusttooeasyforstudentsoverwhelmedbythecrushingburdenofdebttopayoffstudentloans#trumpbudget\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: we’regratefulforourfirstrespondersourrescuepersonnelourfirefightersourpoliceandvolunteerswhohavebeenworkingtirelesslytokeeppeoplesafeprovidemuchneededhelpwhileputtingtheirownlivesontheline\n",
      "Actual party is Republican and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: let’smakeitevengreater#kag🇺🇸\n",
      "Actual party is Republican and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: wehaveabout1hruntilthecavstieuptheseries22im#allin216repbarbaraleeyouscared#roadtovictory\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: congratstobelliottsdonhisnewgigatsdcityhallwearegladyouwillcontinuetoserve…\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: wearereallyclosewehaveover3500raisedtowardthematchrightnowwhootthat’s7000forthenonmathmajorsintheroom😂helpusgetthere\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: todaythecommentperiodforpotus’splantoexpandoffshoredrillingopenedtothepublicyouhave60daysuntilmarch9tosharewhyyouopposetheproposedprogramdirectlywiththetrumpadministrationcommentscanbemadebyemailormail\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: celebratedicseastla’s22yearsofeastsidecommitmentampsalutedcommunityleadersatlastnight’sawardsdinner\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for tweet, party in tweet_data_sample :\n",
    "    estimated_party = classifier.classify(conv_features(tweet,feature_words))\n",
    "    \n",
    "    print(f\"Here's our (cleaned) tweet: {tweet}\")\n",
    "    print(f\"Actual party is {party} and our classifer says {estimated_party}.\")\n",
    "    print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've looked at it some, let's score a bunch and see how we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of counts by actual party and estimated party. \n",
    "# first key is actual, second is estimated\n",
    "parties = ['Republican','Democratic']\n",
    "results = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for p in parties :\n",
    "    for p1 in parties :\n",
    "        results[p][p1] = 0\n",
    "\n",
    "\n",
    "num_to_score = 10000\n",
    "random.shuffle(tweet_data)\n",
    "\n",
    "for idx, tp in enumerate(tweet_data) :\n",
    "    tweet, party = tp    \n",
    "    # Now do the same thing as above, but we store the results rather than printing them\n",
    "    tweet_features = conv_features(tweet, feature_words)\n",
    "   \n",
    "    # get the estimated party\n",
    "    estimated_party = classifier.classify(tweet_features)\n",
    "    \n",
    "    results[party][estimated_party] += 1\n",
    "    \n",
    "    if idx > num_to_score : \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'Republican': defaultdict(int,\n",
       "                         {'Republican': 1, 'Democratic': 4371}),\n",
       "             'Democratic': defaultdict(int,\n",
       "                         {'Republican': 4, 'Democratic': 5626})})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflections\n",
    "\n",
    "democratic parties have better chance of being the correct estimated party, though the classifier accuracy is on the low side in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
